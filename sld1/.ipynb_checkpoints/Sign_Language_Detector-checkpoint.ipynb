{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24abe0b-6030-4465-9632-44c663b889ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474603b3-fd4b-4fef-81c6-4ee64fc95c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec2aa52-299a-4e62-b013-7ad0d1f89191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5c79d3-07c2-4241-afe7-afa08a23103d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    #mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACE_CONNECTIONS) # Draw face connections\n",
    "    #mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311e06ea-0a50-42f4-9ffc-44ef46ce4ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    # mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "    #                          mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "    #                          mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "    #                          ) \n",
    "    # Draw pose connections\n",
    "    # mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "    #                          mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "    #                          mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "    #                          ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d851a8-1598-4fc5-8ae4-083f56137638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    #pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    #face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([lh, rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101c223c-7dd6-4939-b0da-5bb7b7318506",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_DIR = './data'\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "number_of_classes = 10\n",
    "dataset_size = 100\n",
    "actions = np.array(['hello','thanks','yes','iloveyou','Telephone','Family','home','pray','fine','money'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9c6bf6-eb37-4db1-a083-f90984fa6d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    # NEW LOOP\n",
    "    # Loop through actions\n",
    "    for action in actions:\n",
    "        action_dir = os.path.join(DATA_DIR, str(action))\n",
    "        if not os.path.exists(action_dir):\n",
    "            os.makedirs(action_dir)  \n",
    "            # Loop through video length aka sequence length\n",
    "        for frame_num in range(dataset_size):\n",
    "\n",
    "                # Read feed\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "                # Make detections\n",
    "            image, results = mediapipe_detection(frame, holistic)\n",
    "#                 print(results)\n",
    "\n",
    "                # Draw landmarks\n",
    "            draw_styled_landmarks(image, results)\n",
    "                \n",
    "                # NEW Apply wait logic\n",
    "            if frame_num == 0: \n",
    "                cv2.putText(image, 'STARTING COLLECTION', (120,200), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action,frame_num), (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    # Show to screen\n",
    "                cv2.imshow('OpenCV Feed', image)\n",
    "                cv2.waitKey(2000)\n",
    "            else: \n",
    "                cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action,frame_num), (15,12), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                \n",
    "                    # Show to screen\n",
    "                cv2.imshow('OpenCV Feed', image)\n",
    "                \n",
    "                # NEW Export keypoints\n",
    "            keypoints = extract_keypoints(results)\n",
    "            cv2.imwrite(os.path.join(DATA_DIR, str(action), '{}.jpg'.format(frame_num)), frame)\n",
    "\n",
    "                \n",
    "                # Break gracefully\n",
    "            if cv2.waitKey(100) & 0xFF == ord('q'):\n",
    "                break\n",
    "                    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c011735-24d4-4589-b7bc-df1b4a3d0536",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23daec3-5b2e-4f84-8534-b06a49987a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3, max_num_hands=2)\n",
    "\n",
    "DATA_DIR = './data'\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "for dir_ in os.listdir(DATA_DIR):\n",
    "    for img_path in os.listdir(os.path.join(DATA_DIR, dir_)):\n",
    "        img = cv2.imread(os.path.join(DATA_DIR, dir_, img_path))\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        results = hands.process(img_rgb)\n",
    "        \n",
    "        if results.multi_hand_landmarks:\n",
    "            x_ = []\n",
    "            y_ = []\n",
    "            left_hand = np.zeros(21 * 2)  # 21 keypoints, each with (x, y)\n",
    "            right_hand = np.zeros(21 * 2)\n",
    "\n",
    "            for idx, hand_landmarks in enumerate(results.multi_hand_landmarks):\n",
    "                hand_data = []\n",
    "                for landmark in hand_landmarks.landmark:\n",
    "                    hand_data.extend([landmark.x, landmark.y])  # Store (x, y) coordinates\n",
    "\n",
    "                    x_.append(landmark.x)\n",
    "                    y_.append(landmark.y)\n",
    "\n",
    "                # Normalize coordinates\n",
    "                for i in range(len(hand_landmarks.landmark)):\n",
    "                    hand_data[i * 2] -= min(x_)\n",
    "                    hand_data[i * 2 + 1] -= min(y_)\n",
    "\n",
    "                # Assign the first detected hand to left, second to right\n",
    "                if idx == 0:\n",
    "                    left_hand = np.array(hand_data)\n",
    "                elif idx == 1:\n",
    "                    right_hand = np.array(hand_data)\n",
    "\n",
    "            # Concatenate left and right hand into a fixed 42-feature vector\n",
    "            data_aux = np.concatenate([left_hand, right_hand])\n",
    "            data.append(data_aux)\n",
    "            labels.append(dir_)  # Convert label to integer\n",
    "\n",
    "\n",
    "f = open('data.pickle', 'wb')\n",
    "pickle.dump({'data': data, 'labels': labels}, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830a4a7c-3edf-45fa-9ca4-c551bee3b4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "data_dict = pickle.load(open('./data.pickle', 'rb'))\n",
    "\n",
    "data = np.asarray(data_dict['data'])\n",
    "labels = np.asarray(data_dict['labels'])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, shuffle=True, stratify=labels)\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "y_predict = model.predict(x_test)\n",
    "\n",
    "score = accuracy_score(y_predict, y_test)\n",
    "\n",
    "print('{}% of samples were classified correctly !'.format(score * 100))\n",
    "\n",
    "f = open('model.p', 'wb')\n",
    "pickle.dump({'model': model}, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8ba929-dfa6-42ff-b8bc-8c9316f11737",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd870d6a-57ed-4c67-a5dc-72c597de0db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import pickle  # For loading the trained model\n",
    "\n",
    "# Load trained RandomForestClassifier model\n",
    "with open(\"model.p\", \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "    model = model[\"model\"]\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.7, min_tracking_confidence=0.7)\n",
    "\n",
    "# Class labels (Modify based on training dataset)\n",
    "CLASS_NAMES = {0: \"hello\", 1: \"thanks\", 2: \"yes\",3:'iloveyou',4:'Telephone',5:'Family',6:'home',7:'pray',8:'fine',9:'money'}  # Example\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "\n",
    "    # Flip the frame for a mirrored effect\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Convert to RGB for MediaPipe\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(rgb_frame)\n",
    "\n",
    "    # Initialize feature vector with zeros (84 features)\n",
    "    left_hand = np.zeros(42)  # 21 keypoints Ã— (x, y)\n",
    "    right_hand = np.zeros(42)\n",
    "\n",
    "    x_, y_ = [], []\n",
    "\n",
    "    # If hands are detected, extract their landmarks\n",
    "    if results.multi_hand_landmarks:\n",
    "        for idx, hand_landmarks in enumerate(results.multi_hand_landmarks):\n",
    "            hand_data = []\n",
    "\n",
    "            for landmark in hand_landmarks.landmark:\n",
    "                hand_data.append(landmark.x)\n",
    "                hand_data.append(landmark.y)\n",
    "\n",
    "                x_.append(landmark.x)\n",
    "                y_.append(landmark.y)\n",
    "\n",
    "            # Normalize landmarks\n",
    "            for i in range(21):\n",
    "                hand_data[i * 2] -= min(x_)\n",
    "                hand_data[i * 2 + 1] -= min(y_)\n",
    "\n",
    "            # Assign first detected hand to left, second to right\n",
    "            if idx == 0:\n",
    "                left_hand[:] = hand_data\n",
    "            elif idx == 1:\n",
    "                right_hand[:] = hand_data\n",
    "\n",
    "    # Concatenate both hands into a single feature vector (84 features)\n",
    "    features = np.concatenate([left_hand, right_hand])\n",
    "\n",
    "    # Predict using the trained model\n",
    "    prediction = model.predict([features])  # Model expects (1, 84)\n",
    "    predicted_class = CLASS_NAMES.get(prediction[0],prediction[0])\n",
    "\n",
    "    # Draw hand landmarks on the frame\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "    # Display prediction on the frame\n",
    "    cv2.putText(frame, f\"Prediction: {predicted_class}\", (50, 50),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Show the video feed\n",
    "    cv2.imshow(\"Sign Language Detection\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9450e586-ed6a-4747-95d3-2009888b9f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb4474e-f3e5-46ae-bca6-a060132eb16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#debugging for inhomogeneous error\n",
    "count=0\n",
    "for x in data_dict['data']:\n",
    " \n",
    "    if len(x)==84:\n",
    "        print(x.index)\n",
    "        print(count)\n",
    "        count+=1\n",
    "    else:\n",
    "        \n",
    "        print(count)\n",
    "        count+=1\n",
    "        print(data_dict['labels'][263])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7a44b3-de6d-48fd-8b4f-9766b1088e10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sld1",
   "language": "python",
   "name": "sld1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
